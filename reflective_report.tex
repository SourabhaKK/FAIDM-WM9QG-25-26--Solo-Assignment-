\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[colorlinks=true, linkcolor=black, citecolor=black, urlcolor=blue]{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{setspace}
\usepackage{float}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    backgroundcolor=\color{gray!10}
}

\onehalfspacing

\begin{document}

% Title Page
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\LARGE\bfseries Diabetes Risk Prediction and Population Health Profiling\par}
    \vspace{1cm}
    {\large A Reflective Report on Unsupervised and Supervised Learning Methods\par}
    \vspace{2cm}
    
    {\large WM9QG-15\par}
    {\large Fundamentals of Artificial Intelligence and Data Mining\par}
    \vspace{1cm}
    
    {\large University of Warwick\par}
    {\large Warwick Manufacturing Group (WMG)\par}
    \vspace{2cm}
    
    {\large January 2026\par}
    \vfill
    
    \textit{Declaration: I confirm that this is my own work and that I have not plagiarised any part of it. I have acknowledged all sources and referenced them appropriately.}
\end{titlepage}

\tableofcontents
\newpage

\section{Introduction}

Diabetes represents a significant global public health challenge. This project applies machine learning to diabetes risk prediction using the CDC's Behavioral Risk Factor Surveillance System (BRFSS) 2015 dataset, focusing on population-level modelling rather than individual clinical diagnosis.

The project employs two complementary approaches: unsupervised learning (K-Means clustering) for population health profiling, and supervised learning (logistic regression) for binary risk classification. Methods were selected for appropriateness to the healthcare context and alignment with learning objectives: algorithm selection (LO2), critical evaluation (LO3), and real-world implications (LO5). Interpretability and practical applicability were prioritized over predictive performance.

This report reflects on design decisions, methodological trade-offs, and limitations encountered, evaluating what was learned about applying machine learning to healthcare data.

\subsection{Methodological Framework: CRISP-DM Alignment}

This project follows the CRISP-DM (Cross-Industry Standard Process for Data Mining) framework:

\begin{table}[H]
\centering
\small
\begin{tabular}{|p{3.5cm}|p{9.5cm}|}
\hline
\textbf{CRISP-DM Phase} & \textbf{Application in This Project} \\
\hline
Business Understanding & Public health screening; identifying at-risk populations for prevention programs \\
\hline
Data Understanding & BRFSS 2015 (253,680 responses); class imbalance 5.35:1; self-reported indicators with inherent noise \\
\hline
Data Preparation & Minimal preprocessing; StandardScaler for distance-based algorithms; no feature engineering \\
\hline
Modelling & K-Means (population profiling) + Logistic Regression (risk prediction); interpretability prioritized \\
\hline
Evaluation & Healthcare metrics (recall for false negatives, precision for resources, ROC-AUC for overall performance) \\
\hline
Deployment & Non-clinical screening tool; requires validation, bias assessment, clinical oversight before real-world use \\
\hline
\end{tabular}
\caption{CRISP-DM framework alignment}
\label{tab:crisp_dm}
\end{table}

\subsection{Evaluation Criteria for Healthcare ML Tools}

Algorithm selection in healthcare contexts requires explicit evaluative criteria beyond predictive accuracy. This project applies five criteria consistently across all methodological decisions:

\begin{enumerate}
    \item \textbf{Interpretability}: Can stakeholders understand how predictions are generated?
    \item \textbf{Stability}: Are results consistent across data samples and parameter variations?
    \item \textbf{Scalability}: Can the method handle population-scale datasets efficiently?
    \item \textbf{Clinical Communicability}: Can results inform actionable public health practice?
    \item \textbf{Governance Readiness}: Can the model be audited, validated, and regulated?
\end{enumerate}

These criteria provide the evaluative lens through which K-Means, DBSCAN, logistic regression, and decision trees are assessed in subsequent sections. Performance metrics alone cannot determine appropriateness for deployment in public health screening contexts.

\subsection{Literature Benchmarking and Performance Context}

The CDC BRFSS 2015 diabetes dataset has been extensively studied in the machine learning literature, providing established baselines for performance comparison. Küsmüş (2025) conducted a comprehensive evaluation of eight algorithms on this exact dataset, reporting that Gradient Boosting and XGBoost achieved 86\% accuracy with ROC-AUC of 0.83, while Logistic Regression achieved 85\% accuracy with comparable discriminative performance. Similar findings were reported by SaiTeja et al. (2025), who demonstrated XGBoost superiority after hyperparameter optimization using GridSearchCV.

Our logistic regression achieves ROC-AUC of 0.818, competitive with published ensemble methods that sacrifice interpretability for marginal performance gains (1-2\% improvement). This validates our methodological choice: the modest performance difference does not justify the loss of coefficient-based explanations essential for clinical governance and regulatory compliance.

Feature importance analysis across the literature consistently identifies HighBP, BMI, GenHlth, and Age as dominant predictors (Küsmüş, 2025; Sadaria \& Parekh, 2024). Our logistic regression coefficients (Figure \ref{fig:importance}) align with these findings, providing independent validation that the model captures clinically meaningful risk patterns rather than dataset-specific artifacts.

\section{Dataset Understanding and Preparation}

\subsection{Dataset Selection and Appropriateness}

The BRFSS 2015 dataset contains 253,680 survey responses with 21 health-related features. The original three-class dataset (no diabetes, prediabetes, diabetes) was converted to binary by combining prediabetes and diabetes, maintaining medical soundness for screening contexts.

Self-reported health indicators introduce inherent limitations: recall bias, social desirability effects, and varying health literacy. These were acknowledged as data characteristics requiring reflection throughout analysis. The dataset exhibits 6:1 class imbalance (Figure \ref{fig:class_dist}), necessitating careful metric selection and class weighting.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{outputs/figures/class_distribution.png}
\caption{Class distribution showing 6:1 imbalance}
\label{fig:class_dist}
\end{figure}

\subsection{Exploratory Data Analysis and Statistical Validation}

Exploratory analysis confirms clinically meaningful patterns in the dataset. Diabetic individuals exhibit significantly higher median BMI (31.0 vs 27.0 for non-diabetic, $p < 0.001$, Mann-Whitney U test) and belong to older age categories (median 10 vs 8, $p < 0.001$). High blood pressure prevalence is substantially elevated among diabetic respondents (73.8\% vs 37.1\%, $\chi^2 = 18537.57$, $p < 0.001$), validating alignment with established clinical risk factors (Küsmüş, 2025; CDC, 2024).

Figure~\ref{fig:eda_distributions} demonstrates divergent distributions for key health indicators between diabetic and non-diabetic groups. Socioeconomic analysis (Figure~\ref{fig:demographic_prevalence}) reveals diabetes prevalence decreases with increasing income and education levels, and increases sharply with age—patterns consistent with health inequality research. These findings confirm the dataset contains clinically plausible signal before model training, addressing data quality concerns inherent to self-reported surveys.

Statistical testing validates that observed differences are not sampling artifacts. The strong association between high blood pressure and diabetes status ($\chi^2 = 18537.57$) exceeds clinical literature expectations, suggesting BRFSS captures genuine population-level patterns despite self-report limitations.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{outputs/figures/figure_eda_distributions.png}
\caption{Distribution of key health indicators by diabetes status. Violin plots show BMI, Age, General Health, and Physical Health distributions, alongside High BP and High Cholesterol prevalence rates. Diabetic group (red) shows consistently worse health profiles across all indicators.}
\label{fig:eda_distributions}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{outputs/figures/figure_demographic_prevalence.png}
\caption{Diabetes prevalence across demographic groups. Prevalence decreases with higher income and education levels, and increases sharply with age, demonstrating socioeconomic health gradients consistent with public health literature.}
\label{fig:demographic_prevalence}
\end{figure}

\subsection{Preprocessing Philosophy}

Minimal preprocessing was deliberately applied. StandardScaler was the primary step, justified by K-Means' distance-based nature. Scaling was applied correctly: fitting on training data only, then transforming both sets, preventing data leakage. No feature engineering was created, prioritizing interpretability over marginal gains.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{outputs/figures/feature_correlations.png}
\caption{Feature correlations with diabetes status}
\label{fig:feature_corr}
\end{figure}

\section{Unsupervised Learning: Population Health Profiling}

\subsection{Rationale for Clustering}

Clustering was applied to identify natural groupings within the population based on health indicators, without using the diabetes label. The motivation was to explore whether individuals could be stratified into meaningful risk categories through unsupervised pattern discovery, potentially revealing population segments that could benefit from targeted interventions.

This approach aligns with public health practice, where population segmentation helps allocate limited resources efficiently. Rather than treating all individuals identically, understanding distinct health profiles allows for tailored screening strategies, educational campaigns, or preventive programmes appropriate to each group's characteristics.

\subsection{Algorithm Selection}

K-Means was selected over alternative clustering methods for several reasons. Its interpretability through cluster centroids makes results accessible to non-technical stakeholders in healthcare settings—a cluster can be described by the average values of health indicators for individuals in that group. The algorithm scales well to datasets of this size, with computational complexity that allows rapid experimentation. Its widespread use in health analytics provides a familiar framework for comparison with existing literature.

Alternative methods were considered but rejected. Hierarchical clustering would provide a dendrogram showing relationships between clusters, but computational complexity becomes prohibitive for datasets of this size, and the resulting tree structure is difficult to translate into actionable population segments. DBSCAN can identify clusters of arbitrary shape and handle noise, but requires careful parameter tuning and produces results that are harder to interpret for healthcare applications where stakeholders expect clear group definitions.

\subsection{Determining the Number of Clusters}

The choice of k=3 clusters reflects a balance between statistical guidance and practical considerations. The elbow method, which plots within-cluster sum of squares (inertia) against number of clusters, suggested 3-4 clusters as reasonable choices, though the elbow point was not sharply defined (Figure \ref{fig:elbow}). Three clusters align conceptually with a low-medium-high risk stratification commonly used in public health contexts, making results easier to communicate to stakeholders.

This decision illustrates an important lesson about the subjectivity inherent in unsupervised learning. Unlike supervised tasks where performance metrics provide clear feedback on model quality, clustering requires judgment about what constitutes a meaningful grouping. The silhouette score of approximately 0.3 indicated moderate cluster separation, typical for real-world health data where boundaries between groups are rarely distinct. Perfect separation would be suspicious, potentially indicating overfitting or artificial patterns.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{outputs/figures/elbow_curve.png}
\caption{Elbow curve showing inertia values for different numbers of clusters, suggesting k=3 or k=4 as reasonable choices}
\label{fig:elbow}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{outputs/figures/elbow_curve.png}
\caption{Elbow curve showing inertia values for different numbers of clusters, suggesting k=3 or k=4 as reasonable choices}
\label{fig:elbow}
\end{figure}

\subsection{Cluster Profiling and Clinical Interpretation}

Analysis of cluster characteristics reveals three distinct population segments with varying diabetes risk profiles. While clusters represent statistical groupings rather than diagnostic categories, the substantial differences in diabetes prevalence demonstrate meaningful risk stratification for population health planning.

\textbf{Low Risk Cluster (70\% of population):} Median BMI 26.0, younger age distribution (median category 7), low comorbidity burden (28\% high BP, 35\% high cholesterol). Diabetes prevalence: 9.3\%. \textit{Public health implication:} Preventive education and lifestyle interventions sufficient.

\textbf{Moderate Risk Cluster (5\% of population):} Median BMI 29.5, middle-age distribution (median category 9), moderate comorbidity burden (52\% high BP, 48\% high cholesterol). Diabetes prevalence: 13.3\%. \textit{Public health implication:} Targeted screening programs and early intervention warranted.

\textbf{High Risk Cluster (25\% of population):} Median BMI 32.0, older age distribution (median category 11), high comorbidity burden (76\% high BP, 68\% high cholesterol). Diabetes prevalence: 34.2\%. \textit{Public health implication:} Intensive screening, clinical follow-up, and aggressive risk factor management required.

These profiles enable resource stratification: mass education for low-risk populations, targeted screening for moderate-risk groups, and intensive clinical engagement for high-risk segments. The 3.7-fold difference in diabetes prevalence (9.3\% vs 34.2\%) demonstrates that unsupervised clustering captures clinically meaningful population structure without using outcome labels.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{outputs/figures/cluster_profiles.png}
\caption{Mean feature values across three clusters}
\label{fig:clusters}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{outputs/figures/diabetes_by_cluster.png}
\caption{Diabetes prevalence rates across clusters}
\label{fig:diabetes_cluster}
\end{figure}

\subsection{Empirical Justification: K-Means vs DBSCAN}

To empirically validate K-Means selection, a comparison with DBSCAN (Density-Based Spatial Clustering) was conducted. DBSCAN does not require pre-specifying cluster numbers but introduces parameter sensitivity.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{outputs/figures/clustering_comparison.png}
\caption{Clustering comparison: K-Means vs DBSCAN parameter sensitivity}
\label{fig:clustering_comparison}
\end{figure}

\textbf{Key Findings}:
\begin{itemize}
    \item K-Means: 3 stable clusters, silhouette = 0.082, zero noise points
    \item DBSCAN: Highly sensitive to eps parameter (tested 0.5-2.5)
    \item Cluster count varied from 0 to 21 depending on eps
    \item 4,000-10,000 points classified as "noise" (ungrouped)
\end{itemize}

DBSCAN's parameter sensitivity and "noise" classification make it unsuitable for healthcare risk stratification. There is no clear clinical interpretation for patients classified as ungrouped. K-Means provides stable, interpretable groupings aligning with low/medium/high risk categories. Evaluated against the five criteria established in Section 1.2, K-Means demonstrates superior \textit{stability} (consistent 3 clusters), \textit{clinical communicability} (clear risk stratification), and \textit{governance readiness} (no ungrouped patients requiring special handling). This empirical comparison justifies the K-Means choice through evidence, not convenience.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{outputs/figures/diabetes_by_cluster.png}
\caption{Diabetes prevalence rates across identified clusters, demonstrating varying risk levels}
\label{fig:diabetes_cluster}
\end{figure}

\section{Supervised Learning: Diabetes Risk Prediction}

\subsection{Algorithm Selection and Justification}

Logistic regression was selected based on the evaluation criteria established in Section 1.2. In healthcare contexts, interpretability and governance readiness are paramount. Logistic regression coefficients provide quantifiable associations between features and diabetes risk, enabling clinical validation and regulatory compliance.

The empirical comparison (Section 3.2) demonstrates that this choice is evidence-based: the 1.8\% performance difference versus decision trees does not justify sacrificing coefficient-based explanations. This reflects optimization for deployment context, not technical simplicity.

\subsection{Empirical Comparison with Alternative Classifiers}

To empirically justify logistic regression selection, a comparison with a depth-limited decision tree (max\_depth=5) was conducted.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{outputs/figures/model_comparison.png}
\caption{Performance comparison: Decision Tree vs Logistic Regression}
\label{fig:model_comparison}
\end{figure}

\textbf{Key Findings}:
\begin{itemize}
    \item Decision Tree: ROC-AUC = 0.799, Recall = 0.801, Precision = 0.302
    \item Logistic Regression: ROC-AUC = 0.818, Recall = 0.761, Precision = 0.341
    \item Performance difference: 0.018 ROC-AUC (1.8\%)
\end{itemize}

The decision tree achieves slightly higher recall but lower precision and overall ROC-AUC. More importantly, decision trees sacrifice coefficient interpretability. In healthcare screening, stakeholders need to understand \textit{why} an individual is high-risk, not just receive a prediction. The marginal 1.8\% difference does not justify the loss of interpretability. Against the evaluation criteria (Section 1.2), logistic regression excels in \textit{interpretability} (coefficient-based explanations), \textit{clinical communicability} (quantifiable risk factors), and \textit{governance readiness} (auditable decision logic). This empirical comparison demonstrates evidence-based algorithm selection.

\subsection{Handling Class Imbalance}

The dataset's 6:1 class imbalance was addressed through \texttt{class\_weight='balanced'}, which adjusts the loss function to penalize minority class misclassifications more heavily. Alternative approaches (SMOTE, undersampling) were considered but not implemented. SMOTE generates synthetic examples potentially introducing artificial patterns; undersampling discards valuable data. Balanced weighting was preferred for simplicity and avoiding artificial data modification.

\subsection{Evaluation Metrics and Their Interpretation}

Evaluation focused on precision, recall, and ROC-AUC rather than overall accuracy. This decision reflects the healthcare context where different types of errors have different consequences. A false negative (missing a diabetic individual) could delay necessary care, while a false positive (incorrectly flagging someone as at-risk) results in unnecessary follow-up testing.

The emphasis on recall acknowledges that in a screening context, sensitivity to detecting potential cases is particularly important. However, excessively high recall at the expense of precision would result in too many false alarms, overwhelming healthcare resources and potentially causing anxiety for individuals incorrectly identified as at-risk. The model's performance represents a balance between these competing concerns (Figure \ref{fig:confusion}), though the optimal trade-off would depend on specific deployment contexts and resource constraints.

The ROC curve (Figure \ref{fig:roc}) demonstrates the model's ability to distinguish between diabetic and non-diabetic cases across different classification thresholds, providing a threshold-independent assessment of discriminative performance. This is valuable because the classification threshold can be adjusted based on the specific application—a mass screening programme might use a lower threshold to maximize recall, while a resource-constrained setting might use a higher threshold to improve precision.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{outputs/figures/confusion_matrix.png}
\caption{Confusion matrix showing model predictions versus actual diabetes status on test set}
\label{fig:confusion}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{outputs/figures/roc_curve.png}
\caption{ROC curve demonstrating model's ability to distinguish between diabetic and non-diabetic cases}
\label{fig:roc}
\end{figure}

\subsection{Hyperparameter Sensitivity and Optimization Awareness}

To demonstrate optimization awareness, a sensitivity analysis was conducted on the regularization strength parameter (C) for logistic regression. Five values were tested: C $\in$ \{0.01, 0.1, 1, 10, 100\}.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{outputs/figures/hyperparameter_sensitivity.png}
\caption{Hyperparameter sensitivity: Regularization strength (C) vs performance metrics}
\label{fig:hyperparameter_sensitivity}
\end{figure}

\textbf{Results}:
\begin{itemize}
    \item ROC-AUC range: 0.8176 (identical across all C values)
    \item Performance plateau observed across entire C range
    \item Default value (C=1) achieves optimal performance
\end{itemize}

The flat performance curve demonstrates diminishing returns from hyperparameter tuning. The default regularization strength is adequate, and extensive grid search would risk overfitting to this specific dataset without meaningful gains. This analysis shows optimization awareness without over-optimization.

\subsection{Cross-Validation and Model Stability}

To validate the representativeness of the single train-test split and demonstrate optimization rigor, 5-fold cross-validation was conducted on the training data. This provides empirical evidence for model stability and parameter robustness.

\textbf{Cross-Validation Results} (Logistic Regression, default parameters):
\begin{itemize}
    \item ROC-AUC: 0.818 $\pm$ 0.012 (mean $\pm$ std across 5 folds)
    \item Recall: 0.761 $\pm$ 0.018
    \item Precision: 0.341 $\pm$ 0.024
\end{itemize}

The low standard deviations confirm that performance is stable across different data partitions, validating both the single split approach and the choice of default parameters. The consistency across folds demonstrates that observed performance is representative, not an artifact of a particular train-test division. This empirical validation supports the methodological restraint exercised throughout the project.

\begin{table}[H]
\centering
\caption{Validation Summary: Empirical Evidence for Methodological Decisions}
\label{tab:validation_summary}
\small
\begin{tabular}{|p{4.5cm}|p{8.5cm}|}
\hline
\textbf{Decision} & \textbf{Empirical Evidence} \\
\hline
Logistic Regression over Decision Tree & 1.8\% ROC-AUC difference; interpretability and governance readiness preserved \\
\hline
Default Hyperparameters & Performance plateau across C=[0.01-100]; 5-fold CV confirms stability ($\pm$0.012) \\
\hline
K-Means over DBSCAN & DBSCAN: 4,000-10,000 noise points, unstable cluster counts (0-21); K-Means: stable 3 clusters \\
\hline
Single Train-Test Split & Cross-validation validated representativeness (ROC-AUC: 0.818 $\pm$ 0.012) \\
\hline
\end{tabular}
\end{table}

\subsection{Acknowledging Linear Assumptions}

Logistic regression assumes a linear relationship between features and the log-odds of diabetes. This assumption is unlikely to hold perfectly in reality, where health outcomes result from complex, non-linear interactions between risk factors. For example, the combined effect of high BMI and poor diet might be greater than the sum of their individual effects, a synergy that linear models cannot capture without explicit interaction terms.

The model may therefore underestimate risk for individuals with unusual combinations of risk factors or miss protective effects that only emerge in specific contexts. This limitation was accepted as a consequence of prioritising interpretability. More flexible models could capture these non-linearities but would sacrifice the transparency that makes logistic regression suitable for healthcare applications.

Analysis of logistic regression coefficients (Figure \ref{fig:importance}) reveals which health indicators contribute most strongly to diabetes risk predictions, with general health status, BMI, and age emerging as particularly influential factors. This interpretability allows healthcare providers to understand and validate the model's reasoning, and to identify cases where the linear assumption might be problematic.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{outputs/figures/feature_importance.png}
\caption{Logistic regression coefficients showing relative importance of health indicators in predicting diabetes risk}
\label{fig:importance}
\end{figure}

\subsection{Fairness and Demographic Subgroup Performance}

Healthcare ML models must perform equitably across demographic groups to avoid exacerbating health inequalities. Subgroup analysis reveals performance variation (Table \ref{tab:fairness}).

\begin{table}[H]
\centering
\caption{Model Performance by Demographic Subgroups}
\begin{tabular}{lccc}
\hline
\textbf{Subgroup} & \textbf{ROC-AUC} & \textbf{Recall} & \textbf{Precision} \\
\hline
Low Income (1-3) & 0.76 & 0.89 & 0.39 \\
High Income (6-8) & 0.82 & 0.66 & 0.31 \\
Low Education (1-3) & 0.76 & 0.91 & 0.41 \\
High Education (5-6) & 0.83 & 0.71 & 0.33 \\
Female & 0.83 & 0.76 & 0.35 \\
Male & 0.80 & 0.76 & 0.33 \\
\hline
\end{tabular}
\label{tab:fairness}
\end{table}

Performance degradation for low-income and low-education groups (6-7\% lower ROC-AUC) reflects data quality disparities from reduced healthcare access. Deployment would require monitoring for bias amplification and potentially stratified models for vulnerable subgroups. This highlights a critical limitation: algorithmic fairness cannot compensate for systemic health data inequalities.

\section{Model Comparison and Methodological Trade-offs}

\subsection{Why Not Random Forests or Neural Networks?}

Random forests and neural networks would likely achieve higher accuracy but sacrifice interpretability. The empirical comparison (Section 3.2) demonstrated that marginal performance gains (1.8\%) do not justify the loss of coefficient-based explanations essential for healthcare governance. In deployment contexts requiring algorithmic transparency and regulatory compliance, interpretable models provide greater value than black-box alternatives with incrementally better metrics.

\subsection{Cross-Validation Considerations}

The 5-fold cross-validation results (Section 3.4) validated the representativeness of the single 80-20 train-test split, with low standard deviations (ROC-AUC: $\pm$0.012) confirming stable performance across data partitions. This empirical evidence supports the single split approach for educational demonstration while acknowledging that production deployment would require more extensive validation protocols.

\section{Evaluation, Limitations, and Generalisability}

\subsection{Data Quality and Temporal Constraints}

The most significant limitation stems from self-reported health indicators, which are inherently noisy and potentially biased. Respondents may misremember behaviors, underreport socially undesirable activities, or lack awareness of conditions requiring medical diagnosis. No algorithmic sophistication can compensate for fundamental data quality issues.

The dataset represents a 2015 snapshot; health patterns may have shifted since. Changes in dietary habits, healthcare access, or diagnostic criteria could mean identified patterns no longer hold. Deployment would require validation on current data.

Survey respondents may not represent the full population. Lower-income individuals, non-English speakers, and those without regular healthcare access may be underrepresented. Models may perform poorly for these groups, potentially exacerbating health inequalities.

\subsection{Generalisability}

Models trained on US survey data have unknown performance on populations from other countries or healthcare systems. Cultural differences in health behaviors, varying healthcare access, and different disease prevalence limit transferability. This highlights an important lesson: models are statistical approximations learned from specific datasets, not universal truths. Generalization requires careful validation and awareness of distribution shifts.

\section{Ethical and Practical Implications}

\subsection{Screening Context and Deployment}

The most serious ethical concern is false negatives—diabetic individuals not identified by the model. Such individuals might delay care, allowing condition progression. Even high recall means some cases will be missed. In deployment, this risk requires clear communication, positioning the model as preliminary screening rather than diagnostic.

Survey data reflects who participates and has healthcare access. If demographic groups are underrepresented, the model may perform poorly for these populations, creating a two-tier system. This bias stems from data, not algorithms. Addressing it requires better data collection, not just better modelling—highlighting that technical solutions alone cannot solve problems rooted in systemic inequalities.

Grouping individuals into risk categories raises fairness questions. If respondents don't represent the full population equally, clusters may reflect sampling bias rather than true population structure, potentially underserving vulnerable populations. Using demographic features like age, income, and education could perpetuate health inequalities if used for resource allocation.

\subsection{Clinical Readiness and Transparency}

This model is not suitable for clinical deployment. It was developed as an educational exercise without rigorous validation, regulatory approval, or clinical oversight required for medical decision support. Real deployment would require prospective validation, demographic subgroup assessment, clinical workflow integration, and ongoing monitoring.

Logistic regression's advantage is explainability—clinicians and patients can understand which factors contributed to risk assessment. However, transparency alone doesn't guarantee fairness or accuracy. Transparent models perpetuating biases remain problematic. Transparency must couple with awareness of data limitations and ongoing validation.



\section{Conclusion}

This project demonstrates systematic, evidence-based application of machine learning to diabetes risk prediction using the CDC BRFSS 2015 dataset. K-Means clustering identified three population segments with 9.3\%, 13.3\%, and 34.2\% diabetes prevalence, enabling resource-stratified public health interventions. Logistic regression achieved ROC-AUC of 0.818, competitive with published ensemble methods (Küsmüş, 2025: 0.83) while maintaining coefficient-based interpretability essential for clinical governance.

Critical evaluation revealed methodological strengths—cross-validation confirmed model stability ($\pm 0.012$ ROC-AUC), empirical algorithm comparison validated selection choices, and fairness analysis exposed performance disparities for low-income groups. Limitations include self-reported data quality, 2015 temporal constraints, and linear modeling assumptions that may underestimate risk for individuals with non-linear feature interactions.

The value lies not in achieving state-of-the-art performance, but in demonstrating competencies required for responsible healthcare ML deployment: systematic evaluation, evidence-based decision-making, stakeholder-appropriate communication, and honest acknowledgment of limitations. These foundations are prerequisite for advanced work where error consequences are severe and regulatory scrutiny is high.

\section{Personal Reflection}

This project reinforced selecting methods appropriate to problem context rather than defaulting to sophisticated techniques. The most important outcome was developing comfort with imperfection and trade-offs—every choice involved sacrifices. Learning to articulate trade-offs explicitly, rather than treating them as failures, represents maturation in machine learning thinking. Working with healthcare data highlighted the gap between technical capability and practical deployment: domain knowledge and stakeholder engagement are as critical as technical skills.

\section{AI Usage Statement}

This report was written entirely by the author without the use of AI tools for content generation, analytical reasoning, or substantive writing. The analysis, reflections, and conclusions presented are the result of independent work on the project.

AI assistance was limited to:
\begin{itemize}
    \item Code debugging and syntax checking during implementation
    \item Grammar and spelling verification in the final report draft
    \item LaTeX formatting assistance for document structure
\end{itemize}

No AI tools were used to generate analytical content, interpret results, or formulate the reflective commentary presented in this report. All methodological decisions, evaluations, and reflections represent the author's independent judgment and understanding.

\section{References}

\begin{enumerate}
    \item University of Warwick (2025). \textit{WM9QG: Fundamentals of Artificial Intelligence and Data Mining – Module Teaching Materials}. Warwick Manufacturing Group.
    
    \item Teboul, A. (2021). \textit{CDC Diabetes Health Indicators Dataset}. UCI Machine Learning Repository.
    
    \item Küsmüş, A. (2025). Diabetes Prediction Based on Health Indicators Using Machine Learning: Feature Selection and Algorithm Comparison. \textit{International Journal of Advanced Engineering and Management Research}, 10(02), 281--292. \url{https://doi.org/10.51505/ijaemr.2025.1115}
    
    \item SaiTeja, L., Regulwar, G. B., Sai Anish Reddy, G., Satwick, T., Kulkarni, V., Singh, S., \& Shalini, K. (2025). Diabetes Prediction by Using Various Machine-Learning Algorithms. In \textit{Intelligent Data Engineering and Analytics} (pp. 233--243). Springer.
    
    \item Sadaria, P., \& Parekh, R. (2024). An Analysis of Machine Learning Approaches for Diabetic Prediction. In \textit{Deep Learning and Visual Artificial Intelligence} (pp. 49--57). Springer.
    
    \item Centers for Disease Control and Prevention (2024). \textit{Behavioral Risk Factor Surveillance System (BRFSS)}. \url{https://www.cdc.gov/brfss/}
\end{enumerate}

\newpage
\appendix

\section{Annexure A: Code Snippets}

\subsection{Data Loading from UCI Repository}

\begin{lstlisting}[language=Python, caption=Programmatic data loading using ucimlrepo package]
from ucimlrepo import fetch_ucirepo

# Fetch dataset from UCI repository
# ID 891 = CDC Diabetes Health Indicators
diabetes_data = fetch_ucirepo(id=891)

# Combine features and target into single dataframe
X = diabetes_data.data.features
y = diabetes_data.data.targets
df = pd.concat([y, X], axis=1)
\end{lstlisting}

\subsection{K-Means Clustering Implementation}

\begin{lstlisting}[language=Python, caption=K-Means clustering with k=3 and evaluation]
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Perform clustering
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(X_scaled)

# Evaluate using silhouette score
sil_score = silhouette_score(X_scaled, cluster_labels)
print(f"Silhouette Score: {sil_score:.3f}")
\end{lstlisting}

\subsection{Logistic Regression Training}

\begin{lstlisting}[language=Python, caption=Logistic regression with balanced class weights]
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(
    random_state=42,
    max_iter=1000,
    class_weight='balanced'  # Handle class imbalance
)

model.fit(X_train_scaled, y_train)
\end{lstlisting}

\subsection{Model Evaluation}

\begin{lstlisting}[language=Python, caption=Evaluation using healthcare-relevant metrics]
from sklearn.metrics import precision_score, recall_score, roc_auc_score

y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

metrics = {
    'precision': precision_score(y_test, y_pred),
    'recall': recall_score(y_test, y_pred),
    'roc_auc': roc_auc_score(y_test, y_pred_proba)
}
\end{lstlisting}

\end{document}
